{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "PH-0ReGfmX4f",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "JcMwzZxoAimU",
        "8G2x9gOozGDZ",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Rosamann Retails Sales Prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Individual\n",
        "##### **Member 1 -**  - Jayesh Prakash Dahiwale\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rossmann operates over 3,000 drug stores in 7 european countries. So our porject aim is to predict their daily sales for upto six weeks in advance. Stores sales are influenced by many factors, including promotions, competitions,school and state holidays, seasonality and locality. So In this project we have been provided the data from 1,500 Rossmann stores. So our task is to forecast the \"Sales\" column for the test** \n",
        "##<p>Provided there are two datasets, one is **Store** dataset having <b>1,115</b> observations in it with <b>10</b> columns and It gives us static information about each store such as the model and assortment of the store, information about the nearest competitor store, and whether or not they participate in the consecutive promotion \"Promo2\". Largely we're looking at numerical and date data, but Store Type and Assortment are flagged with letters to indicate store models and assorment level, per the variable explanations, as well as the PromoInterval column listing abbreviated months.</p>\n",
        "## <p>Other datatset is about **Sales** dataset having <b>1,017,209</b> observations in it with <b>9</b> columns and It gives us static information about each store such as the model and assortment of the store, information about the nearest competitor store, and whether or not they participate in the consecutive promotion \"Promo2\". Largely we're looking at numerical and date data, but Store Type and Assortment are flagged with letters to indicate store models and assorment level, per the variable explanations, as well as the PromoInterval column listing abbreviated months.</p>"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/Jayeshdahiwale/RegressionCapstoneProjectII"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DataSet Link -**"
      ],
      "metadata": {
        "id": "d1GjmOWgPlgc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Dataset DriveLink:https://drive.google.com/drive/folders/1XtTzgmtM-e9jGHbIq8pLzIbo3C8NF1T2?usp=share_link"
      ],
      "metadata": {
        "id": "sliViF0jPtgu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## **Questions for analysis are as follows**\n",
        "**1) Visualising the distribution of \"Sales\" & \"Customers\"?**<br>\n",
        "**2) Statistics of Sales column ?** <br>\n",
        "**3) Which rows are unnecessary and need to be removed ??** <br>\n",
        "**4) What are the outliers ?** <br>\n",
        "**5) Establishing relationship between Sales and Customers ?** <br>\n",
        "**6) How stores are performing in Sales by month based on Assortment type ?**<br>\n",
        "**7) How UPT metric compares across stores of different assortment types ?**<br>\n",
        "**8)Correlation between competition distance and UPT metric?**<br>\n",
        "**9)Which linear regression model is best?**<br>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Define Your Business Objective?**"
      ],
      "metadata": {
        "id": "PH-0ReGfmX4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increase the number of sales by predicting the rates at at optimal rate and finding the best suitalbe condition which attract the customers thereby increasing the profit for the Drug Store."
      ],
      "metadata": {
        "id": "PhDvGCAqmjP1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Importing the required packages**"
      ],
      "metadata": {
        "id": "IkikXoI50aJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import math"
      ],
      "metadata": {
        "id": "ugsJi7bk0gss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll begin with the required libraries and reading our .csv files"
      ],
      "metadata": {
        "id": "L1NCSUaZ5m8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mount the google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "cgIqljBG1PCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the dataset file\n",
        "# dataset file googgle drive link : https://drive.google.com/file/d/185htr6OyxAZ0xb9QwxHpsUn8eTlq58v5/view?usp=share_link\n",
        "working_directory_path = '/content/drive/MyDrive/MachineLearningAlmabetterJourney/CapstoneProject2/'\n",
        "\n"
      ],
      "metadata": {
        "id": "7fIvHBHO2v1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store_lookup = pd.read_csv(working_directory_path + 'store.csv')"
      ],
      "metadata": {
        "id": "nbTCYrN01D57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_data = pd.read_csv(working_directory_path + 'Rossmann Stores Data.csv')\n"
      ],
      "metadata": {
        "id": "TDx_ehdI5_c_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store_lookup.head()"
      ],
      "metadata": {
        "id": "oVhpTAQY3zXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store_lookup.shape"
      ],
      "metadata": {
        "id": "ivc5bzQD4OQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store_lookup.info()"
      ],
      "metadata": {
        "id": "OH-7vhpq4m26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def null_cols(data):\n",
        "  columns = data.columns\n",
        "  null_cols = [(col,data[col].isna().sum()) for col in columns if data[col].isna().sum()!=0]\n",
        "  return null_cols"
      ],
      "metadata": {
        "id": "MsDr_CsU7Zms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "null_cols(store_lookup)"
      ],
      "metadata": {
        "id": "dhFOywrH7rCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see in store lookup table there are total 6 null columns"
      ],
      "metadata": {
        "id": "bqGg-I-U-MQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets see the sales data\n",
        "sales_data.head()"
      ],
      "metadata": {
        "id": "Vt5JHiIY-uoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_data.shape"
      ],
      "metadata": {
        "id": "dUDiYrYr-8QQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_data.info()"
      ],
      "metadata": {
        "id": "TMUEClAO-_8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These is no null columns in this dataset"
      ],
      "metadata": {
        "id": "5dWEI5wv_Gex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#lets see the starting date and end date of sales data\n",
        "print(sales_data['Date'].min())\n",
        "print(sales_data['Date'].max())"
      ],
      "metadata": {
        "id": "dOQ6bWN__PU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now we will merge the two data resources so we can more easily work with them. We will join the tables baes on the shared store column, which is a foreign key in the sales_data table and primary key in the stores_llokup table, so we'll validate the merge baed on this many to one relationship**"
      ],
      "metadata": {
        "id": "qRy7n95s_lN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_sales = sales_data.merge(store_lookup,how='left',on='Store',validate = 'many_to_one')"
      ],
      "metadata": {
        "id": "yM_V7eZqATXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_sales.info()"
      ],
      "metadata": {
        "id": "KRFeGVDlIojl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_sales.shape"
      ],
      "metadata": {
        "id": "gqOKzdKSB8lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_sales.head()"
      ],
      "metadata": {
        "id": "JCFl4bFzMPaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_sales.describe()"
      ],
      "metadata": {
        "id": "xJw-MQdnCOmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_sales.info()"
      ],
      "metadata": {
        "id": "ob3yKZ5uLQbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lets check the skewness of columns i.e.** <br>\n",
        " \"Day of Week\",\"Sales\",\"Customers\",\"Competion Distance\""
      ],
      "metadata": {
        "id": "bHxk_MpzI8dj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_sales[['DayOfWeek','Sales','Customers','CompetitionDistance']].skew()"
      ],
      "metadata": {
        "id": "CcfRnJIQKJxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can see here competition distance,customers are positively skewed<br>\n"
      ],
      "metadata": {
        "id": "4VK1MjEOKmLl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<p>Lets check the distribution bell peak using kurotsis</p>**"
      ],
      "metadata": {
        "id": "L-PRyyVeLYgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_sales[['DayOfWeek','Sales','Customers','CompetitionDistance']].kurtosis()"
      ],
      "metadata": {
        "id": "TtquasIbLhsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see the that competition distance,and customers has highest peak...And sales and Day of week has negative kurtosis"
      ],
      "metadata": {
        "id": "bWvGhQPgL0Y8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 1: Visualising the distribution of Sales and Customer columns**"
      ],
      "metadata": {
        "id": "zS0D0aA1akS-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **<p>Lets visualize the skewness and kurtosis</p>**"
      ],
      "metadata": {
        "id": "bb0kQwrv5WON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig,axes = plt.subplots(1,2)\n",
        "sns.distplot(merged_sales['Sales'],color=\"y\",ax=axes[0])\n",
        "sns.distplot(merged_sales['Customers'],color=\"y\",ax=axes[1])\n",
        "fig.set_size_inches(15,5)"
      ],
      "metadata": {
        "id": "OYhQqdj-5l5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Histograms of our Sales and Customers values show us a positive skew and high kurtosis."
      ],
      "metadata": {
        "id": "30rQSrXzOGIL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 2: Statistics of Sales column**"
      ],
      "metadata": {
        "id": "rcbs2CCkbnlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <p>**Next we will take a closer look at our statistics for sales column.**</p>"
      ],
      "metadata": {
        "id": "GM-b0tApOq_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"skew     \" + str(round(merged_sales['Sales'].skew(),6)))\n",
        "print(\"kurtosis \" + str(round(merged_sales['Sales'].kurtosis(),6)))\n",
        "print(merged_sales['Sales'].describe().round(3))\n",
        "print(\"mode     \" + str(merged_sales['Sales'].mode()))"
      ],
      "metadata": {
        "id": "3NmGtQq1O8X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see our sales figures have a slightly positive skew, with the **mean (5773.819)** only slightly larger than the **median (5744.000)**, suggesting most outliers are to the right of the mean.\n",
        "\n",
        "High kurtosis indicates it's **leptokurtic** with the likelihood of heavy tails and outliers that may be extreme. Considering our min and max values of **0** and **41,551** sales, we aren't surprised to see there may be some extreme outliers.\n",
        "\n",
        "The max value well above the mean of 5,773.819 and outside the **standard deviation of 3849.926** helps us see how our mean ends up getting pulled slightly to the right for our positive skew.\n",
        "\n",
        "There is no mode as we don't have any stores recording the exact same number of sales on any days, which isn't surprising."
      ],
      "metadata": {
        "id": "Nl2yf2NJShUR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning"
      ],
      "metadata": {
        "id": "SWrvO0PGpxBM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#<p>Datetime</p>"
      ],
      "metadata": {
        "id": "u8buzmTbT2Ur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we'll convert our Date column to datetime for easier filtering later on.\n",
        "\n",
        "With an .info() call we can confirm our Date column was successfully converted to datetime.\n",
        "\n",
        "We'll also expand our Date column into separate Month, Day of Month, and Year columns for easier filtering."
      ],
      "metadata": {
        "id": "-sTgcfcNULew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_sales['Date'] = pd.to_datetime(merged_sales['Date'], format=\"%Y-%m-%d\", errors='raise')"
      ],
      "metadata": {
        "id": "xsXmKAPEbiL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_sales.info()"
      ],
      "metadata": {
        "id": "axALhlqNbuko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_sales[\"Year\"] = merged_sales[\"Date\"].dt.year\n",
        "merged_sales[\"Month\"] = merged_sales[\"Date\"].dt.month\n",
        "merged_sales[\"DayOfMonth\"] = merged_sales[\"Date\"].dt.day"
      ],
      "metadata": {
        "id": "R9c398fhl7H6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Duplicates**"
      ],
      "metadata": {
        "id": "O5SC3w9Imsh7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll do a quick check for and delete any rows that are complete duplicates of another row, as we should only have one entry for each store and date."
      ],
      "metadata": {
        "id": "-YU80oB1lx-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(merged_sales[merged_sales.duplicated()])"
      ],
      "metadata": {
        "id": "I77FQ-y4cw_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text and Expected Value**"
      ],
      "metadata": {
        "id": "XXeGNWbWcr5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_sales.loc[merged_sales['StateHoliday']==0,'StateHoliday'] = '0'"
      ],
      "metadata": {
        "id": "CkRQjPNDPyfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we'll check our four columns that contain strings to make sure we have no inappropriately entered data. Using str.strip() to remove any accidental leading or trailing spaces."
      ],
      "metadata": {
        "id": "ESRDnYAOrSzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in merged_sales:\n",
        "  \n",
        "  if merged_sales[col].dtype == object and col != 'StateHoliday':\n",
        "    merged_sales[col] = merged_sales[col].str.strip()"
      ],
      "metadata": {
        "id": "WHr2oWR8rhlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we'll look at value_counts for those columns and confirm only expected values are found."
      ],
      "metadata": {
        "id": "9WNe_ylDu2JX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in merged_sales:\n",
        "  if merged_sales[col].dtype==object:\n",
        "    print(merged_sales[col].value_counts().sort_index())"
      ],
      "metadata": {
        "id": "ZcrIs_IOvl4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_sales.info()"
      ],
      "metadata": {
        "id": "vvpcL_gmNclh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Everything is looking as expected in our columns that contain text, and because each column only has 3-4 unique values we can see we don't need to worry about changing anything to lower, upper, or proper case."
      ],
      "metadata": {
        "id": "U2DS3X8ufo05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll also do a quick check of columns we expect to only have a few unique values or binary flags, such as DayOfWeek or Promo, to make sure there's nothing unexpected there."
      ],
      "metadata": {
        "id": "OVDCmEeAfs7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "check_cols = [ 'Open', 'Promo', 'Promo2', 'SchoolHoliday',  'DayOfWeek', 'CompetitionOpenSinceMonth',  'CompetitionOpenSinceYear', 'Promo2SinceWeek', 'Promo2SinceYear']\n",
        "for col in check_cols:\n",
        "    print(col)\n",
        "    print(sorted(merged_sales[col].unique()))"
      ],
      "metadata": {
        "id": "StfBkUYJng4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see the relevant binary flags, day of weeks, week, month, and year numeric values we expect.\n",
        "\n",
        "Because the CompetitionOpenSinceMonth, CompetitionOpenSinceYear, Promo2SinceWeek, and Promo2SinceYear columns are only using whole numbers and they are a discrete value, we will change them from floats to integers."
      ],
      "metadata": {
        "id": "UmTzgMBfga7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_sales['CompetitionOpenSinceMonth'] = merged_sales['CompetitionOpenSinceMonth'].convert_dtypes()\n",
        "merged_sales['CompetitionOpenSinceYear'] = merged_sales['CompetitionOpenSinceYear'].convert_dtypes()\n",
        "merged_sales['Promo2SinceWeek'] = merged_sales['Promo2SinceWeek'].convert_dtypes()\n",
        "merged_sales['Promo2SinceYear'] = merged_sales['Promo2SinceYear'].convert_dtypes()"
      ],
      "metadata": {
        "id": "n0712dLF0Z5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_sales.info()"
      ],
      "metadata": {
        "id": "Xz8wpTxHtEyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can see that CompetitionDistance, our only continuous value, is our only column with floats."
      ],
      "metadata": {
        "id": "HUC_iySlhNKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  **Question3):Removing rows and columns**"
      ],
      "metadata": {
        "id": "XrpOYlimhccD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below I look at entries for stores on days they were closed.\n",
        "\n",
        "For the purposes of our analysis I've chosen to drop these rows, as no sales are recorded on days stores are closed. The zero sales recorded for each of these rows lowers the average sales, and we can see this by comparing the mean Sales for all entries in our table to the mean Sales of only days that stores were open. If we filter for entries of stores that are closed we'll see a return of **172,817** rows, all of which record the expected 0 sales, lowering our mean Sales statistic.\n",
        "\n",
        "**The potential information lost here is if we want to compare stores based on the number of days they are open or closed**, but that is beyond the scope of our analysis for now. To avoid losing this information we will make a copy of our dataframe with only the days stores are open, to further be referred to as sales, rather than altering merged_sales in case we wish to access this data at a later time."
      ],
      "metadata": {
        "id": "UFNUqh_6tge1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## mean sales including entries for days stores are closed\n",
        "merged_sales['Sales'].mean()"
      ],
      "metadata": {
        "id": "FJevtCjmrkiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Not lets check the mean only for the data when the store is open\n",
        "merged_sales[merged_sales['Open']==1].Sales.mean()"
      ],
      "metadata": {
        "id": "ULvR_6ibru4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## confirming all entries where the store is  marked as closed have 0 sales\n",
        "merged_sales.loc[merged_sales[\"Open\"] == 0, ['Sales', 'Customers']].value_counts()\n"
      ],
      "metadata": {
        "id": "TqHvADQ7l9uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## creating new sales dataframe with only entries for days stores are open\n",
        "sales = merged_sales.drop(index=(merged_sales[merged_sales[\"Open\"] == 0]).index, axis=1)"
      ],
      "metadata": {
        "id": "DAezRm4ikPHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now as we chose to delete any rows for days a store wasn't open, our Open column should only contain the value 1 now and is redundant, so we'll remove that"
      ],
      "metadata": {
        "id": "wgVK8K2Tn3Zn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales.drop(columns=[\"Open\"], inplace=True)"
      ],
      "metadata": {
        "id": "qk0IiieMnTjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 4 : What are the outliers in the dataset ?**\n"
      ],
      "metadata": {
        "id": "nsDOoznDudq7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we'll take a look at any outliers we may need to treat."
      ],
      "metadata": {
        "id": "vbBxSyiuwjdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales.plot(y=['Sales', 'Customers', 'CompetitionDistance'], \n",
        "           kind='box', subplots=True, layout=(2,2), figsize=(15,15))"
      ],
      "metadata": {
        "id": "ju6buTsTwilP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the box plots above we can see that Sales, Customers, and CompetitionDistance all appear to have significant outliers, so we'll explore further by calculating and investigating the outliers for each one.\n"
      ],
      "metadata": {
        "id": "ftFnOAYl69JZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sales Outliers**\n",
        "\n"
      ],
      "metadata": {
        "id": "McXJZPey7PvI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will start with the Sales column."
      ],
      "metadata": {
        "id": "88iTmLQy7cXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_outlier(df,column): ## function for calculating outliers\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    IQR = Q3 - Q1\n",
        "    lower = Q1 - 1.5 * IQR\n",
        "    upper = Q3 + 1.5 * IQR\n",
        "    percent_outliers = round(((df[df[column] > upper].shape[0]) + (df[df[column] < lower].shape[0])) / df.shape[0] * 100, 2)\n",
        "    return lower, upper, percent_outliers"
      ],
      "metadata": {
        "id": "yh40DQzJz5oc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col = 'Sales'\n",
        "lower_sales, upper_sales, percent_outliers_sales = calculate_outlier(sales, col)\n",
        "\n",
        "print(\"lower band = \" + str(lower_sales))\n",
        "print(\"upper band = \" + str(upper_sales))\n",
        "print(\"percentage of sales that are outliers = \" + str(percent_outliers_sales) + \"%\")\n",
        "print('Number of observations in which sales leass than <0 are '+str(len(sales[sales.Sales <0])))"
      ],
      "metadata": {
        "id": "mD7r7M_Y7-BM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We know from our summary statistics that there aren't any sales below 0, so we'll just look at the upper outliers that we've calculated for the Sales column."
      ],
      "metadata": {
        "id": "6a4VioXl9AQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales[sales[col] > upper_sales]"
      ],
      "metadata": {
        "id": "dRMsGC9c8-v4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While 30,769 is a lot of values, we can see from our calculte_outlier function that these outliers only account for 3.64% of all our sales values.\n",
        "\n",
        "We'll look further to see if we see any trends with the outliers based on Month or Type of Store."
      ],
      "metadata": {
        "id": "YAonXm8m-g--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_outliers_by_month = pd.pivot_table((sales.loc[sales[col] > upper_sales]), index='Month', values='Sales', aggfunc='count')\n",
        "\n",
        "sales_outliers_by_month.plot(y='Sales', kind='bar', figsize=(10,5), title=\"# of Sales Outlier Entries by Month\")\n",
        "plt.legend(loc='upper right', bbox_to_anchor=(1.2, 1.1))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZTEqSeek_Rdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_outliers_by_stype = pd.pivot_table((sales.loc[sales[col] > upper_sales]), index='StoreType', values='Sales', aggfunc='count')\n",
        "\n",
        "sales_outliers_by_stype.plot(y='Sales', kind='bar', figsize=(6,6), \n",
        "                             title=\"# of Sales Outlier Entries by Store Type\", \n",
        "                             color=['blue','red','green','orange'])\n",
        "plt.ylabel('Sales')\n",
        "plt.legend(loc='upper right', bbox_to_anchor=(1.2, 1.1))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zQPask_e_ric"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we look at the Sales outliers by month, we see the most represented month is **December at 17.33%**, which is unsurprising given the Christmas holidays. However, when we look at the outliers by Store Type we see that the **61.71% majority** are coming from **Type A stores**, while **Type B, C, D** are more equally represented at **11-15%**. This suggests that Type A stores may be the best performers in regards to outstanding sales days, and is worth looking into further.\n",
        "\n",
        "Below we will treat our Sales outliers by imputing them with our upper range value we calculated earlier, **13611.5**, rounded up to **13612** as our Sales column is a measure of discrete values using whole numbers. As these outliers represent exceptionally high sales day, they are intended to be high numbers, but we would like to treat the outliers to limit their influence on any future modelling. As such imputing with our upper range value feels more appropriate than using our mean Sales value.\n",
        "\n",
        "We also save this a new dataframe going forward, to further be referenced to as sales_treated, so that we can preserve our sales dataframe with the outliers intact, should we wish to investigate them further."
      ],
      "metadata": {
        "id": "wAfRRtWzBNwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_treated = sales.copy()"
      ],
      "metadata": {
        "id": "R97QHxguDh6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_treated.loc[sales_treated[col] > upper_sales, 'Sales'] = 13612"
      ],
      "metadata": {
        "id": "p71tdj1iDmTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Further check whether our imputation worked or not"
      ],
      "metadata": {
        "id": "t7WqAoUhDxp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_treated[sales_treated['Sales'] > 13612]"
      ],
      "metadata": {
        "id": "vC25dYv6DsFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Customer Outliers**"
      ],
      "metadata": {
        "id": "odTSly_50F8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we'll calculate and investigate our Customer outliers."
      ],
      "metadata": {
        "id": "Io4mWZOYFBwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col = 'Customers'\n",
        "lower_cust, upper_cust, percent_outliers_cust = calculate_outlier(sales_treated, col)\n",
        "\n",
        "print(str(lower_cust) + \", \" + str(upper_cust) +\", \" + str(percent_outliers_cust) + \"%\")\n",
        "print('Number of rows when customer < 0 : ' + str(len(sales[sales_treated['Customers']<0])))"
      ],
      "metadata": {
        "id": "XPM0V5zM0Y4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to Sales, we know from our summary statistics that we don't have any Customer values below 0, so we'll just look at our upper range value."
      ],
      "metadata": {
        "id": "J5pGx4ksFseP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_treated[sales_treated['Customers'] > upper_cust]"
      ],
      "metadata": {
        "id": "7WQ9qkEhrLJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We Can see right away that several of these entries have a **Sales value of 13,612**, which we know to be our newly imputed upper range value for Sales outliers. We expect a high correlation between Customers driving Sales, so we'll check to see how much crossover we have between our Sales and Customers outliers.\\"
      ],
      "metadata": {
        "id": "JsqwfsMkG2td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_treated[(sales_treated['Customers'] > upper_cust) & (sales_treated['Sales'] == 13612)]"
      ],
      "metadata": {
        "id": "nc5gemo91G6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see a crossover of **21,420 rows**, or approximately **52%** of our Customer outlier entries are also Sales outlier entries.\n",
        "\n",
        "We will also investigate how these Customer outliers break down by Month and StoreType just as we did with our Sales outliers."
      ],
      "metadata": {
        "id": "krVzFB6dHJlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cust_outliers_by_month = pd.pivot_table((sales_treated.loc[sales_treated[col] > upper_cust]), index='Month', values='Customers', aggfunc='count')\n",
        "\n",
        "cust_outliers_by_month.plot(y='Customers', kind='bar', figsize=(10,5), title=\"# of Customer Outlier Entries by Month\")\n",
        "plt.legend(loc='upper right', bbox_to_anchor=(1.2, 1.1))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rBG30TWSw5p3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cust_outliers_by_stype = pd.pivot_table((sales_treated.loc[sales_treated[col] > upper_cust]), index='StoreType', values='Customers', aggfunc='count')\n",
        "\n",
        "cust_outliers_by_stype.plot(y='Customers', kind='bar', figsize=(6,6), \n",
        "                             title=\"# of Customer Outlier Entries by Store Type\", \n",
        "                             color=['blue','red','green','orange'])\n",
        "plt.legend(loc='upper right', bbox_to_anchor=(1.2, 1.1))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wLPPaqe4H2XV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**December** is our most represented month for Customer outliers, similar to our Sales outliers, but it's percentage of outliers is less than we saw with our Sales.\n",
        "\n",
        "We also see **store Type A** with the strongest showing when we break down the outliers by store type. Much like the Sales outliers **Type A stores** represent a **strong 60%+** of the outliers. Surprisingly, **Type D stores** represent a tiny **1.02%** of these Customer outliers, where as they represented the second largest percentage of Sales outliers at **15.29%**. Further investigation into the number of items bought (Sales) per transaction (Customer) may prove insightful.\n",
        "\n",
        "Similar to our Sales outliers, we will also limit our Customer outliers to our calculated upper range, by imputing them to **1,454,** so as to limit their influence but also indicate that they're meant to be high numbers."
      ],
      "metadata": {
        "id": "M5R5EBqf4V-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_treated.loc[sales_treated['Customers'] > upper_cust, 'Customers'] = 1454"
      ],
      "metadata": {
        "id": "n2ygrKwS50ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_treated.loc[sales_treated['Customers'] > upper_cust, 'Customers']"
      ],
      "metadata": {
        "id": "6r23YgHn6dlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Missing Values**"
      ],
      "metadata": {
        "id": "u6KxEw2H6zWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we'll take a look at missing values. We'll start by assessing how many we have and where."
      ],
      "metadata": {
        "id": "7e_175Ay7cZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_treated.isna().sum()"
      ],
      "metadata": {
        "id": "uxNtOrd17ikU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(sales_treated.isna().sum() * 100 / sales_treated.shape[0]).round(2)   ## missing values as a % of all values in the column"
      ],
      "metadata": {
        "id": "BMKHpJ63UnBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While we see a rather high 50% missing values rate for Promo2SinceWeek and Promo2SinceYear we can check and see if these all just correspond to stores that aren't running Promo2, and hence would not be expected to have valid data for these columns."
      ],
      "metadata": {
        "id": "PkZD2kBw9WJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(sales_treated[sales_treated['Promo2'] == 0].shape[0])\n",
        "print(sales_treated['Promo2SinceWeek'].isna().sum())\n",
        "print(sales_treated['Promo2SinceYear'].isna().sum())"
      ],
      "metadata": {
        "id": "c64Rd_FLU8gC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the number of entries with missing **Promo2SinceWeek** and **Promo2SinceYear** values corresponds with the number of entries where the store isn't participating in Promo2, we can rest assured that the lack of data here is appropriate.\n",
        "\n"
      ],
      "metadata": {
        "id": "iEhCN49DVjld"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The other area we see missing data is in relation to competition stores, mostly in the two columns relating when the nearest competitor store opened, and to much lesser extent in **CompetitionDistance**\n",
        "\n",
        "The CompetitionDistance values may be used for analyzing how stores perform based on how close their competition is, and as such missing values could skew such analysis. 0 would be an inappropriate replacement for these null values as it would indicate that the competition stores are incredibly close. As such we'll look to replace these missing values with the mean CompetitionDistance based on the StoreType."
      ],
      "metadata": {
        "id": "D_Nb-BN-Vl_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_type_a = round(store_lookup.loc[(store_lookup['StoreType'] == 'a'), 'CompetitionDistance'].mean(), 1)\n",
        "mean_type_b = round(store_lookup.loc[(store_lookup['StoreType'] == 'b'), 'CompetitionDistance'].mean(), 1)\n",
        "mean_type_c = round(store_lookup.loc[(store_lookup['StoreType'] == 'c'), 'CompetitionDistance'].mean(), 1)\n",
        "mean_type_d = round(store_lookup.loc[(store_lookup['StoreType'] == 'd'), 'CompetitionDistance'].mean(), 1)\n",
        "\n",
        "print(\"The mean Compeition Distance for stores of type A is \" + str(mean_type_a))\n",
        "print(\"The mean Compeition Distance for stores of type B is \" + str(mean_type_b))\n",
        "print(\"The mean Compeition Distance for stores of type C is \" + str(mean_type_c))\n",
        "print(\"The mean Compeition Distance for stores of type D is \" + str(mean_type_d))"
      ],
      "metadata": {
        "id": "RaGrCzqg-ul7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_treated.loc[sales_treated['StoreType'] == 'a'] = sales_treated.loc[sales_treated['StoreType'] == 'a'].fillna(value={\"CompetitionDistance\" : mean_type_a})\n",
        "sales_treated.loc[sales_treated['StoreType'] == 'b'] = sales_treated.loc[sales_treated['StoreType'] == 'b'].fillna(value={\"CompetitionDistance\" : mean_type_b}) \n",
        "sales_treated.loc[sales_treated['StoreType'] == 'c'] = sales_treated.loc[sales_treated['StoreType'] == 'c'].fillna(value={\"CompetitionDistance\" : mean_type_c}) \n",
        "sales_treated.loc[sales_treated['StoreType'] == 'd'] = sales_treated.loc[sales_treated['StoreType'] == 'd'].fillna(value={\"CompetitionDistance\" : mean_type_d}) "
      ],
      "metadata": {
        "id": "CN01dUKG_QDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_treated.isna().sum()"
      ],
      "metadata": {
        "id": "m7xZMl4QaOLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will replace the null values in our PromoInterval column. As this column contains strings listing the months the Promo2 starts anew, we will replace the missing values with a string 'NA' for Not Applicable."
      ],
      "metadata": {
        "id": "6Bx27h56ibLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_treated.loc[sales_treated['PromoInterval'].isna(), 'PromoInterval'] = \"NA\""
      ],
      "metadata": {
        "id": "sUugW7JSAXlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_treated['PromoInterval'].value_counts()"
      ],
      "metadata": {
        "id": "nFYJ5PPrBlPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the missing values in **'CompetitionOpenSinceYear'** & **'Promo2SinceYear'** it would be inappropriate to replace them with zeros, as further calculations with the years in that column could lead to values suggesting a competition store has been open **2022** years. As such, we'll impute these missing values with the current year, so if calculations are done to find how long the nearest competition store has been open since these instances of no competition store nearby will return zero years.\n",
        "\n",
        "In the same manner we will impute the missing values in **'CompetitionOpenSinceMonth'** with the **current month**, and 'Promo2SinceWeek' with the current week."
      ],
      "metadata": {
        "id": "xh0b1jyLi8IY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_treated.loc[sales_treated['CompetitionOpenSinceYear'].isna(), 'CompetitionOpenSinceYear'] = dt.datetime.now().year\n",
        "\n",
        "sales_treated.loc[sales_treated['Promo2SinceYear'].isna(), 'Promo2SinceYear'] = dt.datetime.now().year\n",
        "\n",
        "sales_treated.loc[sales_treated['CompetitionOpenSinceMonth'].isna(), 'CompetitionOpenSinceMonth'] = dt.datetime.now().month\n",
        "\n",
        "sales_treated.loc[sales_treated['Promo2SinceWeek'].isna(), 'Promo2SinceWeek'] = dt.datetime.now().isocalendar()[1]"
      ],
      "metadata": {
        "id": "z382KVAIl3uO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets see the null counts\n",
        "sales_treated.isna().sum()"
      ],
      "metadata": {
        "id": "RPAqH4ZlD4ZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 5: Establising relationship between Sales and Customers ?**"
      ],
      "metadata": {
        "id": "78ur1XcNm80v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Adding an UPT column**"
      ],
      "metadata": {
        "id": "P3hilI_MEtKh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on our investigation of Sales and Costumers outliers, particularly how they're represented by different store types, I thought it would be worthwhile to add a calculated column to give us some insight into the relationship between Customers and Sales for each store and day."
      ],
      "metadata": {
        "id": "eVsvcm9sqG3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(sales_treated['Customers'] >= sales_treated['Sales']).value_counts()"
      ],
      "metadata": {
        "id": "pIVvd14eFQG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that for almost all entries we have in our dataframe, the number of Sales at a given store is greater than the number of Customers. Suggesting that the Customers value is derived by how many transactions there are at a store, and the Sales value is indicative of how many individual items are sold. Thus we can calculate the average number of items sold for each transaction as Units Per Transaction (UPT).\n",
        "\n",
        "First we'll quickly investigate the 54 rows where there aren't more Sales than Customers."
      ],
      "metadata": {
        "id": "NgSNO_pKqgRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_treated[sales_treated['Customers'] >= sales_treated['Sales']]"
      ],
      "metadata": {
        "id": "Nsiu_-seH0iO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see the majority of these days are entries with both zero Sales and zero Customers recorded. This seems odd for a day that the store is open. A quick check of merged_sales, which still has the Open column, gives us the same results and assures us that the stores are indeed marked as open on these days."
      ],
      "metadata": {
        "id": "jVIWaQQbKtMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_sales[(merged_sales['Customers'] >= merged_sales['Sales']) & merged_sales['Open'] == 1]"
      ],
      "metadata": {
        "id": "-nULPE4OK0UG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I can only speculate as to why we have entries for stores that are open but aren't recording any sales, perhaps a stocktake day? Let's also look at the cases that aren't zero Sales and zero Customers."
      ],
      "metadata": {
        "id": "2VHMuDPxLC3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_treated[sales_treated['Customers'] > sales_treated['Sales']]"
      ],
      "metadata": {
        "id": "YwHHjv9HLI21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'm unsure of why we have two days with zero Sales and a small handful of Customers. As I have no explanation for these 2 days, nor the 52 other open days with zero Sales and Customers, I don't feel comfortable deleting them.\n",
        "\n",
        "This does pose a small problem for calculating our average UPT, however. As such we will create our UPT column by dividing the day's Sales by the days Customers to find the average Units Per Transaction for each day and store. The resulting 52 null values will be imputed with a zero to reflect the zero Sales for those entries."
      ],
      "metadata": {
        "id": "VLNesK-DLdLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_treated['UPT'] = sales_treated['Sales'] / sales_treated['Customers']"
      ],
      "metadata": {
        "id": "nAei2LLmLh7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_treated['UPT'].isna().sum()"
      ],
      "metadata": {
        "id": "GG7dxouDr-aN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_treated.loc[sales_treated['UPT'].isna(), 'UPT'] = 0"
      ],
      "metadata": {
        "id": "MaPUOGiVsHBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_treated['UPT'].isna().sum()"
      ],
      "metadata": {
        "id": "i3-VMm2NsI16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll also add this UPT column to our sales table which still contains our Sales and Customers outliers, so we can explore the UPT with the outliers as well."
      ],
      "metadata": {
        "id": "DjgjiveTsUsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales['UPT'] = sales['Sales'] / sales['Customers']\n",
        "sales.loc[sales['UPT'].isna(), 'UPT'] = 0"
      ],
      "metadata": {
        "id": "sP7io__QsTwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Review of Summary Statistics Post Cleaning**"
      ],
      "metadata": {
        "id": "lZOY2DaSsiWE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's quickly review some of our summary statistics now that we've cleaned our data a bit. We can look at the summary statistics for sales to see them as they were before treating outliers and imputing for missing data, and compare them to the sales_treated summary statistics to see how they've changed."
      ],
      "metadata": {
        "id": "sIk7E_GSNJk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_sales[['Sales', 'Customers', 'CompetitionDistance']].describe() ## BEFORE cleaning\n"
      ],
      "metadata": {
        "id": "gfmzCJ6cSimq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_treated[['Sales', 'Customers', 'CompetitionDistance', 'UPT']].describe() ## AFTER cleaning"
      ],
      "metadata": {
        "id": "XtOu94JvSwYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the cleaning has narrowed our standard deviation for Sales and Customers, as well as raising the mean after removing the entries for closed stores."
      ],
      "metadata": {
        "id": "K3eRutlltLJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_sales[['Sales', 'Customers', 'CompetitionDistance']].skew() ## BEFORE cleaning"
      ],
      "metadata": {
        "id": "X5KP_ym9OwZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_treated[['Sales', 'Customers', 'CompetitionDistance', 'UPT']].skew() ## AFTER cleaning"
      ],
      "metadata": {
        "id": "4AwphE9IsOv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_sales[['Sales', 'Customers', 'CompetitionDistance']].kurtosis() ## BEFORE cleaning"
      ],
      "metadata": {
        "id": "RQ4azBb6Wmpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_treated[['Sales', 'Customers', 'CompetitionDistance', 'UPT']].kurtosis() ## AFTER cleaning"
      ],
      "metadata": {
        "id": "Gt7zQYBUwjro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig,axes = plt.subplots(1,2)\n",
        "sns.distplot(sales_treated['Sales'],color=\"y\",ax=axes[0])\n",
        "sns.distplot(sales_treated['Customers'],color=\"y\",ax=axes[1])\n",
        "fig.set_size_inches(15,5)"
      ],
      "metadata": {
        "id": "6mDnx_jbuLOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Histograms of our Sales and Customers values shows us a slight positive skew, which changed a little after our cleaning, and we see a more significant change in kurtosis being lowered. We also see the effect of imputing our outliers with our upper range limit on the right side of either histogram.\n",
        "\n",
        " we now got results that are looking far closer to a standard normalization."
      ],
      "metadata": {
        "id": "93TCrEcqunl3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exploratory Insights**"
      ],
      "metadata": {
        "id": "4y8-zSAZZsew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 6: How stores perform in Sales by month based of Assortment type?**"
      ],
      "metadata": {
        "id": "MbWMtl8_50b-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's explore how stores perform in Sales by month, based on Assortment type. We know that Assortment type A offers a \"basic\" assortment of merchandise, Type B offers and \"extra\" assortment, and type C offers an \"extended\" assortment.\n",
        "\n",
        "Because our data ranges from Jan. 1, 2013 - July 31, 2015, we will exclude the 2015 data for now so as we are only looking at a complete years' worth of numbers."
      ],
      "metadata": {
        "id": "OntWsc3MvVjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assortment_pivot_total_sales = pd.pivot_table((sales_treated[sales_treated['Year'] < 2015]), ## Excluding 2015 data\n",
        "               index='Month', values='Sales', columns='Assortment', aggfunc=np.sum)\n",
        "assortment_pivot_total_sales"
      ],
      "metadata": {
        "id": "lpdUrvUdZytE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assortment_pivot_total_sales.plot(kind='line', title='Total Sales by Month and Store Assortment', figsize=(7,8), grid=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8oC7kBsIwfkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A quick look at Sales by volume of total sales shows that stores of Assortment types A and C have significantly more volume than type B stores. Type B stores stay fairly consistent in total Sales volume across all months, with minor upticks during mid-year and end year. Type A and C stores can be seen to follow very similar trends in terms of Sales volume.\n",
        "\n",
        "A quick look at how many stores we have of each Assortment type will show us significantly less stores of Assortment type B, which accounts for the significantly lower volume of Sales."
      ],
      "metadata": {
        "id": "6QPgLgzCwxUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.pivot_table((sales_treated[sales_treated['Year'] < 2015]), index='Assortment', values='Store', aggfunc='count')"
      ],
      "metadata": {
        "id": "TYtU3Sxma9Wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Due to the vast differences in total Sales volume based on Assortment type, lets also look at the average number of Sales.\n",
        "\n",
        "(Note: We could include our 2015 data since we're calculating the mean Sales now, but for the sake of consistency when comparing it with the total Sales we will continue to use the same 2013-2014 data.)"
      ],
      "metadata": {
        "id": "bGFpU2BHdamc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assortment_pivot_avg_sales = pd.pivot_table((sales_treated[sales_treated['Year'] < 2015]), index='Month', values='Sales', columns='Assortment', aggfunc=np.mean)\n",
        "assortment_pivot_avg_sales"
      ],
      "metadata": {
        "id": "8u3VumVzc6VM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assortment_pivot_avg_sales.plot(kind='line', title='Average Sales by Month and Store Assortment', figsize=(7,5), grid=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EMxc0JWpo1QU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look at average Sales by store Assortment type we can see that stores of type B actually perform quite well when compared to types A and C, despite there being significantly less type B stores! Types A and C continue to follow very similar trends for Sales, but Type C stores consistently outperform type A stores.\n",
        "\n"
      ],
      "metadata": {
        "id": "AWezEoclyVxO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 7: How UPT metric compares across stores of different assortment types ?**\n",
        "\n",
        "Let's also take a look at how our UPT metric compares across stores of different Assortment types.\n",
        "\n",
        "(Note: Because we included a UPT metric for our table that still has our extremely high Sales outliers, we will plot that too, to compare.)"
      ],
      "metadata": {
        "id": "8osl0vv86NH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assortment_pivot_avg_UPT = pd.pivot_table((sales_treated[sales_treated['Year'] < 2015]), \n",
        "                                     index='Month', values='UPT', columns='Assortment', aggfunc=np.mean)\n",
        "assortment_pivot_avg_UPT_outliers = pd.pivot_table((sales[sales['Year'] < 2015]), \n",
        "                                     index='Month', values='UPT', columns='Assortment', aggfunc=np.mean)\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2)\n",
        "\n",
        "assortment_pivot_avg_UPT.plot(kind='line', title='Average UPT by Month and Store Assortment (no outliers)', figsize=(7,5), grid=True, ax=axes[0])\n",
        "assortment_pivot_avg_UPT_outliers.plot(kind='line', title='Average UPT by Month and Store Assortment (outliers incl.)', figsize=(7,5), grid=True, ax=axes[1])\n",
        "fig.set_size_inches(15, 5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lCXAKVdZ7MSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see stores of Assortment type C are our best performers for UPT. Comparing UPT with and without outliers treated, we can see that with the outliers treated we can more clearly see upward and downward trends, whereas with outliers included these trends look less impactful."
      ],
      "metadata": {
        "id": "PG0aJ6mU71Jp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploring out Sales and Customers outliers prompted us to create out UPT metric when comparing them by StoreType, so let's explore Sales and UPT by month and StoreType as well.\n",
        "\n",
        "Let's start with a look at how many stores of each StoreType we have."
      ],
      "metadata": {
        "id": "80RyIc5BzMhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.pivot_table((sales_treated[sales_treated['Year'] < 2015]), index='StoreType', values='Store', aggfunc='count')"
      ],
      "metadata": {
        "id": "IGUY83Xa8Jkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next let's look at average Sales per month, broken down by StoreType."
      ],
      "metadata": {
        "id": "AyCk8lmC-Izu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stype_pivot_avg_sales = pd.pivot_table((sales_treated[sales_treated['Year'] < 2015]), ## Excluding 2015 data\n",
        "               index='Month', values='Sales', columns='StoreType', aggfunc=np.mean)\n",
        "stype_pivot_avg_sales"
      ],
      "metadata": {
        "id": "xn7decWH-Zyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stype_pivot_avg_sales.plot(kind='line', title='Average Sales by Month and Store Type', figsize=(7,5), grid=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vF5jaEHfV5Gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that stores of type A, C, and D closely follow very similar trends, whereas stores of type B significantly outperform them when it comes to the average number of Sales.\n",
        "\n",
        "Now let's see how UPT compares across Store Types."
      ],
      "metadata": {
        "id": "IDk-Hdekzw4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stype_pivot_avg_UPT = pd.pivot_table((sales_treated[sales_treated['Year'] < 2015]), \n",
        "                                     index='Month', values='UPT', columns='StoreType', aggfunc=np.mean)\n",
        "stype_pivot_avg_UPT_outliers = pd.pivot_table((sales[sales['Year'] < 2015]), \n",
        "                                     index='Month', values='UPT', columns='StoreType', aggfunc=np.mean)\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2)\n",
        "\n",
        "stype_pivot_avg_UPT.plot(kind='line', title='Average UPT by Month and Store Type (no outliers)', figsize=(7,5), grid=True, ax=axes[0])\n",
        "stype_pivot_avg_UPT_outliers.plot(kind='line', title='Average UPT by Month and Store Type (outliers incl.)', figsize=(7,5), grid=True, ax=axes[1])\n",
        "fig.set_size_inches(15, 5)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1x1gBwWrV5_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to our look at UPT across stores of different Assortment types, we can see that treating the outliers leaves us with a graph that more clearly shows changes in the trend of monthly sales. As we look at average UPT by StoreType, we see that while they follow similar trends, store of StoreType D see customers purchasing approximately 3 more items per purchase on average than at a type A or C store, and approximately 6 more items per purchase on average than customers at type B stores.\n",
        "\n",
        "It seems customers of type B stores buy less items per purchase on average, but overall type B stores see more sales. It would reason that type B stores must see more customers on average to account for high average sales.\n",
        "\n",
        "Below we look at total customers and average customers by month. We know of the different store types that there are the fewest stores of type B, so we aren't surprised to see them at the bottom of the Total Customers chart. However, if we look at average customers by store type we see that they average far more customers than the other store types."
      ],
      "metadata": {
        "id": "DK7iWvMyWlaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stype_pivot_total_cust = pd.pivot_table((sales_treated[sales_treated['Year'] < 2015]), index='Month', columns='StoreType', values='Customers', aggfunc='count')\n",
        "stype_pivot_avg_cust = pd.pivot_table((sales_treated[sales_treated['Year'] < 2015]), index='Month', columns='StoreType', values='Customers', aggfunc=np.mean)\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2)\n",
        "\n",
        "stype_pivot_total_cust.plot(kind='line', title='Total Customers by Month and Store Type', figsize=(7,5), grid=True, ax=axes[0])\n",
        "stype_pivot_avg_cust.plot(kind='line', title='Average Customers by Month and Store Type', figsize=(7,5), grid=True, ax=axes[1])\n",
        "fig.set_size_inches(15, 5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B-X9tNE3WqKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand a bit more about our different Store Types, let's also quickly see how they compare in relation to the Competition Distance.\n"
      ],
      "metadata": {
        "id": "hab1fO61a55M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stype_pivot_avg_compdist = pd.pivot_table((sales_treated[sales_treated['Year'] < 2015]),\n",
        "                                         index='StoreType', values='CompetitionDistance', aggfunc=np.mean)\n",
        "\n",
        "stype_pivot_avg_compdist.plot(kind='bar', rot=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9iMb8IeylvvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stores of Store Type B are significantly closer to their nearest competitor store, on average. This might suggest the Type B stores are most often in dense urban shopping areas. Perhaps with many other stores available in closer proximity, customers are less likely but multiple items at the store when they can more easily purchase additional items at other nearby stores. Let's see if there's any correlation between CompetitionDistance and UPT."
      ],
      "metadata": {
        "id": "Islpz_qT1Hzg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 8 : Correlation between competition distance and UPT metric?**"
      ],
      "metadata": {
        "id": "xuno4NkS6z_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_treated.plot(x='CompetitionDistance', y='UPT', kind='scatter', figsize=(8,6))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w1kQ40gtu8lX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We don't see a strong positive or negative correlation between CompetitionDistance and UPT on our scatter plot, but we do see what look to be outliers in our UPT metric."
      ],
      "metadata": {
        "id": "pX2o374D1qo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corr = sales_treated.corr()\n",
        "\n",
        "plt.figure(figsize = (15,8))\n",
        "sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, center=0, cmap=\"YlGnBu\", annot=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6wwlhWul1whi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the correlation calculations above we don't see any meaningful correlations with CompetitionDistance. The notable correlations are a strong positive correlation between Customers and Sales, which isn't surprising. In addition, we see a smaller but positive correlation between Promo and Sales."
      ],
      "metadata": {
        "id": "yV_OA7WyvIy8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EDA Conclusion**"
      ],
      "metadata": {
        "id": "uZlVunlwkNZt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1)**When looking at key performance indicators across store Assortments and Store Types we see they follow similar monthly trends, but numbers can vary by Assortment and Store Type.\n",
        "\n",
        "**2)**When looking at store Assortment we found that stores of Assortment Type B represent a small share of total sales, which isnt surprising considering less than 1% of stores are of the Type B assortment. However, when looking at average monthly sales we see the Assortment Type B stores out performing types A and C by a large margin. Looking at the number of units sold per transaction, UPT, we see Assortment Type A and C stores performing better than Type B.\n",
        "\n",
        "**3)**A similar investigation into sales by Store Type also saw noticeable differences between Store Type B and Store Types A, C, and D. Again, Type B is the least common Store Type, representing only 1.8% of stores sampled from 2013-2014. And yet, similar to Assortment Type B, Store Type B also vastly outperforms other Store Types when looking at average monthly sales. Following the similarities, it also lags to the bottom when we compare UPT amongst Store Types.\n",
        "\n",
        "**4)**In trying to further investigate that difference between average sales and average UPT for Store Type B, we looked at average customers by Store Type and found Type B to be well ahead of the others. This suggests that Type B stores on average have more customers visit, but they buy a smaller number of items. In looking at the average distance of the nearest competitor by store type we found that Type B stores were far closer to their nearest competitor store on average. This lead us to hypothesize that Type B stores may be concentrated in dense urban areas that see more foot traffic.\n",
        "\n",
        "**5)**Further along that line of investigation, we looked for any correlations with competition distance, but found no strong correlations. Looking at correlations across our complete data set we only found the expected strong correlation between Customers and Sales, and smaller correlations between Promo and Sales.\n",
        "\n",
        "**6)**We conclude that well Store Type B and store Assortment B represent a very small sample of the stores, they significantly outperform other store types and assortments in average sales, despite a lower UPT, and see a high volume of customers. It could be worthwhile investigating expanding into more Type B Stores and Assortments.\n",
        "\n"
      ],
      "metadata": {
        "id": "r_dDbdB0kQe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_treated.info()"
      ],
      "metadata": {
        "id": "xhm8ysdhLlKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_treated = sales_treated.astype({\"Promo2SinceWeek\": int,'Promo2SinceYear':int,'CompetitionOpenSinceMonth':int,'CompetitionOpenSinceYear':int})"
      ],
      "metadata": {
        "id": "-44dKNz_a1B5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_treated.info()"
      ],
      "metadata": {
        "id": "H6yhktQZv5PY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets take out the categorical variable names\n"
      ],
      "metadata": {
        "id": "IJF3mFLsL51s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_cols = sales_treated.select_dtypes(include=['object']).columns\n",
        "cat_cols"
      ],
      "metadata": {
        "id": "BMQV0mpiL-4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets check the value counts of each columns\n",
        "for col in cat_cols:\n",
        "  print(sales_treated[col].value_counts().sort_index())"
      ],
      "metadata": {
        "id": "HP1QMeSuNJNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lest change the values of Promointerval column like Feb,May,Aug,Nov - Feb-Nov"
      ],
      "metadata": {
        "id": "09WZs2x8RNhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_val = sales_treated['PromoInterval'].unique()\n",
        "unique_val"
      ],
      "metadata": {
        "id": "E53YIyW_Rdvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for val in unique_val[1:]:\n",
        "  name=val.split(',')\n",
        "  new_name = name[0]+'-'+name[-1]\n",
        "  sales_treated.loc[sales_treated['PromoInterval']==val,'PromoInterval']=new_name\n"
      ],
      "metadata": {
        "id": "KN13XsACRcaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_treated['PromoInterval'].unique()"
      ],
      "metadata": {
        "id": "GNZUqHToW8Zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets one hotencode the categorical variable and save it as different database \"reg_dataset\"\n",
        "reg_dataset = pd.get_dummies(sales_treated, columns=cat_cols, prefix=[pref for pref in cat_cols])"
      ],
      "metadata": {
        "id": "6s6WJ8lxXlMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_treated.columns"
      ],
      "metadata": {
        "id": "HyOn1LcYV-sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_treated.info()"
      ],
      "metadata": {
        "id": "11G-ZqfyYi_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we have the Day of month , month and year as the separate column lets first drop the Date column"
      ],
      "metadata": {
        "id": "ChAJcbPVY-bw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reg_dataset.drop('Date',inplace=True,axis = 1)"
      ],
      "metadata": {
        "id": "xyqFR7nkYjKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_dataset.info()"
      ],
      "metadata": {
        "id": "gHt6ckHEZwjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Lets apply the linear regression model taking whole data into consideration without identifying feature correlation**"
      ],
      "metadata": {
        "id": "4Kyr5zOysYsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score,mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "metadata": {
        "id": "7IoBiWp_t9Wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One Fold regression model"
      ],
      "metadata": {
        "id": "NtwWubuuqz_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_regression(data):\n",
        "  # Get the dependent and independent variable\n",
        "  dependent_col = 'Sales'\n",
        "  independent_cols = set(data.columns) - {dependent_col}\n",
        "  X = data.loc[:,independent_cols].values\n",
        "  y = data.loc[:,dependent_col].values.reshape(-1,1)\n",
        "  # lets split the data into training set and test set\n",
        "  X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)\n",
        "  # lets feature scale the data\n",
        "  scaler = MinMaxScaler()\n",
        "  X_train = scaler.fit_transform(X_train)\n",
        "  X_test = scaler.transform(X_test)\n",
        "  # lets fit the data into Linear regression model\n",
        "  regression = LinearRegression()\n",
        "  regression.fit(X_train,y_train)\n",
        "  y_train_pred = regression.predict(X_train)\n",
        "  y_test_pred = regression.predict(X_test)\n",
        "  # lets check the mean squared error\n",
        "  mse_train = mean_squared_error(y_train,y_train_pred)\n",
        "  mse_test = mean_squared_error(y_test,y_test_pred)\n",
        "  print(f'MSE train :{mse_train}')\n",
        "  print(f'MSE test : {mse_test}')\n",
        "  # lets see the root mean squared error\n",
        "  rmse_train = np.sqrt(mse_train)\n",
        "  rmse_test = np.sqrt(mse_test)\n",
        "  print(f'RMSE train :{rmse_train}')\n",
        "  print(f'RMSE test : {rmse_test}')\n",
        "  r2 = r2_score(y_train,y_train_pred)\n",
        "  print(\"R2_train :\" ,r2)\n",
        "  print(\"Adjusted R2_train : \",1-(1-r2_score((y_train),(y_train_pred)))*((X_train.shape[0]-1)/(X_train.shape[0]-X_train.shape[1]-1)))\n",
        "  r2 = r2_score(y_test,y_test_pred)\n",
        "  print(\"R2_test :\" ,r2)\n",
        "  print(\"Adjusted R2_test : \",1-(1-r2_score((y_test),(y_test_pred)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))\n",
        "  return X,y\n"
      ],
      "metadata": {
        "id": "w-z0sONBq2yW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the dependent and independent variable\n",
        "dependent_col = 'Sales'\n",
        "independent_cols = set(reg_dataset.columns) - {dependent_col}\n",
        "X = reg_dataset.loc[:,independent_cols].values\n",
        "y = reg_dataset.loc[:,dependent_col].values.reshape(-1,1)"
      ],
      "metadata": {
        "id": "SHwW8YSwabRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets split the data into training set and test set\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)"
      ],
      "metadata": {
        "id": "e68NzhyYthNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets feature scale the data\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "KSiineZau8PC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets fit the data into Linear regression model\n",
        "regression = LinearRegression()\n",
        "regression.fit(X_train,y_train)\n",
        "y_train_pred = regression.predict(X_train)\n",
        "y_test_pred = regression.predict(X_test)"
      ],
      "metadata": {
        "id": "FdmoNBHgu_-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets check the mean squared error\n",
        "mse_train = mean_squared_error(y_train,y_train_pred)\n",
        "mse_test = mean_squared_error(y_test,y_test_pred)\n",
        "print(f'MSE train :{mse_train}')\n",
        "print(f'MSE test : {mse_test}')"
      ],
      "metadata": {
        "id": "yWaatRKovLiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets see the root mean squared error\n",
        "rmse_train = np.sqrt(mse_train)\n",
        "rmse_test = np.sqrt(mse_test)\n",
        "print(f'RMSE train :{rmse_train}')\n",
        "print(f'RMSE test : {rmse_test}')"
      ],
      "metadata": {
        "id": "wZp6axf3vVeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we got the, RMS value lets compare these error with the mean value of Sales column"
      ],
      "metadata": {
        "id": "TI5kukfevcqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.mean()"
      ],
      "metadata": {
        "id": "b3Qr5_6BvpRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lest see what is the proportion of the error wrt mean value"
      ],
      "metadata": {
        "id": "t9Qs5BNMv6_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(rmse_test/y_test.mean())*100"
      ],
      "metadata": {
        "id": "TC8Peh8-wAHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can see the predicted mean data error is 7.26% of the mean value. Lets check the r2 score"
      ],
      "metadata": {
        "id": "QDPm2YMAwLmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r2 = r2_score(y_train,y_train_pred)\n",
        "print(\"R2_train :\" ,r2)\n",
        "print(\"Adjusted R2_train : \",1-(1-r2_score((y_train),(y_train_pred)))*((X_train.shape[0]-1)/(X_train.shape[0]-X_train.shape[1]-1)))\n",
        "r2 = r2_score(y_test,y_test_pred)\n",
        "print(\"R2_test :\" ,r2)\n",
        "print(\"Adjusted R2_test : \",1-(1-r2_score((y_test),(y_test_pred)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))"
      ],
      "metadata": {
        "id": "IOszAvNpwbUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **We can see that the r2_score for both training set and test set is nearly similar and r2_score value is also close to 1...From this we can conclude that our model not only peroformed well on training set but also performed well on test dataset.. And it shows that the model is not underfitting as well as not overfitting.**"
      ],
      "metadata": {
        "id": "J0n9jewgw_1Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**While splitting the data we can get such a split which may give us the good result by so to avoid such by chance result lets split up tha data in 5 part and then apply the regression model and then avarge the mean_squred error and check**"
      ],
      "metadata": {
        "id": "zRek0BorzuLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_split = np.array_split(X,5)\n",
        "y_split = np.array_split(y,5)"
      ],
      "metadata": {
        "id": "3WxY_mkZz1yJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def metrics_score(test_data,pred_data,mse_values,rmse_values,r2_score_values,mae_values):\n",
        "  mse = mean_squared_error(test_data,pred_data)\n",
        "  rmse = math.sqrt(mean_squared_error(test_data,pred_data))\n",
        "  r2_score_ = r2_score(test_data,pred_data)\n",
        "  mae = np.mean(np.abs(test_data-pred_data))\n",
        "  print(f'MSE is {mse}')\n",
        "  print(f\"RMSE is {rmse}\")\n",
        "  print(f\"r2score is {r2_score_}\")\n",
        "  print(f'MAE is {mae}')\n",
        "  mse_values.append(mse)\n",
        "  rmse_values.append(rmse)\n",
        "  r2_score_values.append(r2_score_)\n",
        "  mae_values.append(mae)\n",
        "  "
      ],
      "metadata": {
        "id": "wLbbVqtJIn9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets make the regression function"
      ],
      "metadata": {
        "id": "fqITsSTB4oJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def regression_func(X,y):\n",
        "  mse_values = [] # mean squared error\n",
        "  rmse_values=[] # root mean squared error\n",
        "  r2_score_values = [] # r2_score\n",
        "  mae_values = [] # mean absolute error\n",
        "  for i in range(len(X)):\n",
        "    X_test = X[i]\n",
        "    y_test = y[i]\n",
        "    X_train = []\n",
        "    y_train = []\n",
        "    for j,data in enumerate(X):\n",
        "      if i!=j:\n",
        "        X_train.append(data)\n",
        "        y_train.append(y[j])\n",
        "    X_train = np.concatenate(X_train,axis=0)\n",
        "    y_train = np.concatenate(y_train,axis=0)\n",
        "    regressor = LinearRegression()\n",
        "    regressor.fit(X_train,y_train)\n",
        "    y_pred = regressor.predict(X_test)\n",
        "    print(f'For {i+1}st Fold')\n",
        "    metrics_score(y_test,y_pred,mse_values,rmse_values,r2_score_values,mae_values)\n",
        "  print(f'Final Result:')\n",
        "  print(f'Mean Squared Error; {np.mean(mse_values)}')\n",
        "  print(f'Root Mean Squared Error: {np.mean(rmse_values)}')\n",
        "  print(f'R2 Score Values: {np.mean(r2_score_values)}')\n",
        "  print(f'Mean Absolute Error: {np.mean(mae_values)}')\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Np6Aya_Y4qk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As I have already splitted X and y in 5 different dataset. Lets run cross fold validation on each fold"
      ],
      "metadata": {
        "id": "-hkVL92TTkKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regression_func(X_split,y_split)"
      ],
      "metadata": {
        "id": "xtcAp436TFl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From above information we can see the on every fold we got the approx 0.96 and the Avg. r2 score is 0.96 which is represent good model for prediction**"
      ],
      "metadata": {
        "id": "GbCXQ8mHVYl2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Now lets have a look at the correlated features**"
      ],
      "metadata": {
        "id": "GvvVIZXbdERT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exclude_col = 'Sales'\n",
        "include_col = set(sales_treated.columns)-{exclude_col}\n",
        "corr = sales_treated[include_col].corr()\n",
        "\n",
        "plt.figure(figsize = (15,8))\n",
        "sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, center=0, cmap=\"YlGnBu\", annot=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5pLBfj-4VWN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can observe that columns **Promo2sinceWeek** & **Promo2SinceYear**,**Promo2sinceYear** and **Promo2**,**Promo2** and **Promo2sinceweek** showing some kind of correlation... Lets plot and see them visually"
      ],
      "metadata": {
        "id": "rkB1kn4JeysM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correlated_columns = ['Promo2SinceWeek','Promo2SinceYear','Promo2']\n",
        "fig,axes = plt.subplots(nrows=2,ncols=2,figsize=(15,15))\n",
        "for i,col in enumerate(correlated_columns[:-1]):\n",
        "  for j in range(i+1,len(correlated_columns)):\n",
        "    feature = sales_treated[col]\n",
        "    label = sales_treated[correlated_columns[j]]\n",
        "    correlation = feature.corr(label)\n",
        "    axes[i][j-1].scatter(x=feature, y=label)\n",
        "    axes[i][j-1].set(xlabel=col, ylabel=correlated_columns[j])\n",
        "    axes[i][j-1].set_title(correlated_columns[j]+ ' vs ' + col + '- correlation: ' + str(correlation))\n",
        "    z = np.polyfit(sales_treated[col], sales_treated[correlated_columns[j]], 1)\n",
        "    y_hat = np.poly1d(z)(sales_treated[col])\n",
        "\n",
        "    axes[i][j-1].plot(sales_treated[col], y_hat, \"r--\", lw=1)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5mhpKpgQfn5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets check the variance inflation factor for these 3 columns.. [\"Promo2\",\"Promo2SinceYear\",\"Promo2SinceWeek\"]"
      ],
      "metadata": {
        "id": "UatHxsqQnUjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "def calc_vif(X):\n",
        "\n",
        "    # Calculating VIF\n",
        "    vif = pd.DataFrame()\n",
        "    vif[\"variables\"] = X.columns\n",
        "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "    return(vif)"
      ],
      "metadata": {
        "id": "1qiZMEVzyqSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(calc_vif(sales_treated[['Promo2','Promo2SinceYear','Promo2SinceWeek']]))\n",
        "print(sales_treated['Promo2'].corr(sales_treated['Promo2SinceWeek']))\n",
        "print(sales_treated['Promo2'].corr(sales_treated['Promo2SinceYear']))"
      ],
      "metadata": {
        "id": "3fVdmtePoUA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from the above results I can see that Promo2 has the highest variance inflation factor...And even promo2 has significant positive corrlation with the Promo2Since week and very high negative correlation so.... I have decided to drop the promo2 column from regression_data and make a new copy of it"
      ],
      "metadata": {
        "id": "BgWgScN8pE0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reg_data_clean = reg_dataset.copy()\n",
        "reg_data_clean.drop(columns=['Promo2'],inplace=True)\n"
      ],
      "metadata": {
        "id": "VgVisGxBlK8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_data_clean.info()"
      ],
      "metadata": {
        "id": "JynAgLOoqWLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <p>Lets apply the simple linear regression model on this cleaned dataset </p>"
      ],
      "metadata": {
        "id": "n-7GzE3TsKEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X,y = simple_regression(reg_data_clean)"
      ],
      "metadata": {
        "id": "3KulFvl7sWIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## From above results we can see that there is not much significant difference on the result compared to the previous results if we go by performance ..But if feature selection is our main aim in that case we should go with this model..."
      ],
      "metadata": {
        "id": "7i0UTrjSstku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " <p>Lets run 5 fold cross validation</p>"
      ],
      "metadata": {
        "id": "k-ozxEbztBwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_split = np.array_split(X,5)\n",
        "y_split = np.array_split(y,5)"
      ],
      "metadata": {
        "id": "DYJ1ZriStAky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regression_func(X_split,y_split)"
      ],
      "metadata": {
        "id": "Kh1jZwaftioJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lets apply the regularization model..."
      ],
      "metadata": {
        "id": "8qJ3y1ZFt1fC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lasso Regression**\n",
        "We know that Lasso regularization is used to penalize the regression coefficints in such a way that the model should not be overfitted and to avoid multicollinerity by doing the feature selection by making the regression coefficeint of unnecessary feature close to zero sometimes even = 0. In lasso regularization, model tries to minimize the loss fucntion using aboslute sum of regression weights"
      ],
      "metadata": {
        "id": "f0QoV_aQt9WK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets apply the hyperparameter tuning and try to find out the suitable alpha using **GridSearchCV**<br>\n"
      ],
      "metadata": {
        "id": "LvRYLs8o5eea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.experimental import enable_halving_search_cv \n",
        "from sklearn.model_selection import GridSearchCV,HalvingGridSearchCV\n",
        "from sklearn.linear_model import Lasso"
      ],
      "metadata": {
        "id": "fQtI-z2y5o3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dependent_col = 'Sales'\n",
        "independent_cols = set(reg_dataset.columns) - {dependent_col}\n",
        "X = reg_dataset.loc[:,independent_cols].values\n",
        "y = reg_dataset.loc[:,dependent_col].values.reshape(-1,1)\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "5u4ZCsfS588o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cross validation\n",
        "lasso = Lasso()\n",
        "parameters = {'alpha': [0.0039,0.004,0.0041]}\n",
        "lasso_regressor = GridSearchCV(lasso, parameters, scoring='neg_mean_squared_error', cv=5)\n",
        "lasso_regressor.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "6WnjhRBz51Kn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The best fit alpha value is found out to be :\" ,lasso_regressor.best_params_)\n",
        "print(\"\\nUsing \",lasso_regressor.best_params_, \" the negative mean squared error is: \", lasso_regressor.best_score_)"
      ],
      "metadata": {
        "id": "jKJLgeZKAaAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets see no of features included in best model\n",
        "lasso_regressor.n_features_in_"
      ],
      "metadata": {
        "id": "eJcN98Q4ML_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# So lets apply the lasso regularization on reg_dataset without dropping promo2 column\n",
        "# Get the dependent and independent variable\n",
        "def lasso_regression(data,alpha):\n",
        "  # Get the dependent and independent variable\n",
        "  dependent_col = 'Sales'\n",
        "  independent_cols = list(set(data.columns) - {dependent_col})\n",
        "  X = data.loc[:,independent_cols].values\n",
        "  y = data.loc[:,dependent_col].values.reshape(-1,1)\n",
        "  # lets split the data into training set and test set\n",
        "  X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)\n",
        "  # lets feature scale the data\n",
        "  scaler = MinMaxScaler()\n",
        "  X_train = scaler.fit_transform(X_train)\n",
        "  X_test = scaler.transform(X_test)\n",
        "  # lets fit the data into Linear regression model\n",
        "  lasso  = Lasso(alpha=alpha , max_iter= 3000)\n",
        "  lasso.fit(X_train,y_train)\n",
        "  y_train_pred = lasso.predict(X_train)\n",
        "  y_test_pred = lasso.predict(X_test)\n",
        "  # lets get the lasso score\n",
        "  lasso_train = lasso.score(X_train,y_train)\n",
        "  print(f'Lasso Score for Training Set : {lasso_train}')\n",
        "  # lets check the mean squared error\n",
        "  mse_train = mean_squared_error(y_train,y_train_pred)\n",
        "  mse_test = mean_squared_error(y_test,y_test_pred)\n",
        "  print(f'MSE train :{mse_train}')\n",
        "  print(f'MSE test : {mse_test}')\n",
        "  # lets see the root mean squared error\n",
        "  rmse_train = np.sqrt(mse_train)\n",
        "  rmse_test = np.sqrt(mse_test)\n",
        "  print(f'RMSE train :{rmse_train}')\n",
        "  print(f'RMSE test : {rmse_test}')\n",
        "  r2_train = r2_score(y_train,y_train_pred)\n",
        "  print(\"R2_train :\" ,r2)\n",
        "  print(\"Adjusted R2_train : \",1-(1-r2_score((y_train),(y_train_pred)))*((X_train.shape[0]-1)/(X_train.shape[0]-X_train.shape[1]-1)))\n",
        "  r2_test = r2_score(y_test,y_test_pred)\n",
        "  print(\"R2_test :\" ,r2)\n",
        "  print(\"Adjusted R2_test : \",1-(1-r2_score((y_test),(y_test_pred)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))\n",
        "  return lasso_train,mse_train,mse_test,rmse_train,rmse_test,r2_train,r2_test,lasso.coef_,independent_cols"
      ],
      "metadata": {
        "id": "HRQ0yuq1NmQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lets run the lasso regression with alpha=0.004\n",
        "lasso_score,mse_train,mse_test,rmse_train,rmse_test,r2_train,r2_test,coef,columns = lasso_regression(reg_dataset,0.004)\n"
      ],
      "metadata": {
        "id": "rCgAQFF_t8Fj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets check the lasso regression coef\n",
        "cols = columns\n",
        "df = pd.DataFrame(list(zip(cols,coef)),columns =['features','coeficients'])"
      ],
      "metadata": {
        "id": "WKIup58EXEo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "z1RW2_5bbfkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the Lasso has unselected some unrelevant features by making the coef_ = 0.. And more than that Lasso performed better than Simple linear regression"
      ],
      "metadata": {
        "id": "PJYPRgs-bm8b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lets See With Ridge Regression**"
      ],
      "metadata": {
        "id": "mMJsQn5vcgS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge"
      ],
      "metadata": {
        "id": "F70tpVk1dwjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cross validation\n",
        "ridge = Ridge()\n",
        "parameters = {'alpha': [1e-15,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1,5,10,20,30,40,45,50,55,60,100]}\n",
        "ridge_regressor = GridSearchCV(ridge, parameters, scoring='neg_mean_squared_error', cv=5)\n",
        "ridge_regressor.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "gGq9rqd7gPtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The best fit alpha value is found out to be :\" ,ridge_regressor.best_params_)\n",
        "print(\"\\nUsing \",ridge_regressor.best_params_, \" the negative mean squared error is: \", ridge_regressor.best_score_)"
      ],
      "metadata": {
        "id": "9ML1XZ9wgUAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cross validation\n",
        "ridge = Ridge()\n",
        "parameters = {'alpha': [1,2,3,4,5,6]}\n",
        "ridge_regressor = GridSearchCV(ridge, parameters, scoring='neg_mean_squared_error', cv=5)\n",
        "ridge_regressor.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "tuzKi2eyb5Rd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The best fit alpha value is found out to be :\" ,ridge_regressor.best_params_)\n",
        "print(\"\\nUsing \",ridge_regressor.best_params_, \" the negative mean squared error is: \", ridge_regressor.best_score_)"
      ],
      "metadata": {
        "id": "yQQ8BjZReuqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <p> So we got the  best alpha parameter for ridge regression is 4 </p>"
      ],
      "metadata": {
        "id": "miO25LPZgotT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the ridge regression model with alpha = 4"
      ],
      "metadata": {
        "id": "4b_vb17OgzjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ridge_regression(data,alpha):\n",
        "  # Get the dependent and independent variable\n",
        "  dependent_col = 'Sales'\n",
        "  independent_cols = list(set(data.columns) - {dependent_col})\n",
        "  X = data.loc[:,independent_cols].values\n",
        "  y = data.loc[:,dependent_col].values.reshape(-1,1)\n",
        "  # lets split the data into training set and test set\n",
        "  X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)\n",
        "  # lets feature scale the data\n",
        "  scaler = MinMaxScaler()\n",
        "  X_train = scaler.fit_transform(X_train)\n",
        "  X_test = scaler.transform(X_test)\n",
        "  # lets fit the data into Linear regression model\n",
        "  ridge  = Ridge(alpha=alpha , max_iter= 3000)\n",
        "  ridge.fit(X_train,y_train)\n",
        "  y_train_pred = ridge.predict(X_train)\n",
        "  y_test_pred = ridge.predict(X_test)\n",
        "  # lets get the lasso score\n",
        "  ridge_score = ridge.score(X_train,y_train)\n",
        "  print(f'Ridge Score for Training Set : {ridge_score}')\n",
        "  # lets check the mean squared error\n",
        "  mse_train = mean_squared_error(y_train,y_train_pred)\n",
        "  mse_test = mean_squared_error(y_test,y_test_pred)\n",
        "  print(f'MSE train :{mse_train}')\n",
        "  print(f'MSE test : {mse_test}')\n",
        "  # lets see the root mean squared error\n",
        "  rmse_train = np.sqrt(mse_train)\n",
        "  rmse_test = np.sqrt(mse_test)\n",
        "  print(f'RMSE train :{rmse_train}')\n",
        "  print(f'RMSE test : {rmse_test}')\n",
        "  r2_train = r2_score(y_train,y_train_pred)\n",
        "  print(\"R2_train :\" ,r2)\n",
        "  print(\"Adjusted R2_train : \",1-(1-r2_score((y_train),(y_train_pred)))*((X_train.shape[0]-1)/(X_train.shape[0]-X_train.shape[1]-1)))\n",
        "  r2_test = r2_score(y_test,y_test_pred)\n",
        "  print(\"R2_test :\" ,r2)\n",
        "  print(\"Adjusted R2_test : \",1-(1-r2_score((y_test),(y_test_pred)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))\n",
        "  return ridge_score,mse_train,mse_test,rmse_train,rmse_test,r2_train,r2_test,ridge.coef_,independent_cols"
      ],
      "metadata": {
        "id": "zYsXbfaJgw7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets run the ridge regression with alpha= 4\n",
        "ridge_score,mse_train,mse_test,rmse_train,rmse_test,r2_train,r2_test,coef,columns = ridge_regression(reg_dataset,4)\n"
      ],
      "metadata": {
        "id": "GUDlSgOShdzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coef[0]"
      ],
      "metadata": {
        "id": "9NjLDteBittX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols = columns\n",
        "df = pd.DataFrame(list(zip(cols,coef[0])),columns =['features','coeficients'])\n",
        "df"
      ],
      "metadata": {
        "id": "XhLWSHs-iXAl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}